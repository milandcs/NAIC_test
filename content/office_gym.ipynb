{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71fd4343",
   "metadata": {},
   "source": [
    "# An introduction to q-learning\n",
    "\n",
    "In this notebook, we will try to help a newly returned-to-the-office SINTEF-employee find their way to the printer in the renovated offices using reinforcement learning. \n",
    "\n",
    "The goal of the employee (agent) is to find the most efficient route from their office to the printer.\n",
    "\n",
    "The employee does not have a map of the office, and must explore to find a solution.\n",
    "\n",
    "For every step the employee takes that does not end their search, they receive a reward of -1.\n",
    "\n",
    "If the employee finds the printer, they receive a reward of 10. The episode ends.\n",
    "\n",
    "Renovation of the offices is not yet complete, and if the employee enters a construction area, they must immediately return to their office and make a report in Lydia. This yields a reward of -10. The episode ends.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa116e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import office_gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d178ea23",
   "metadata": {},
   "source": [
    "In the file `office_gym.py` we find the `OfficeGym` class. An instance of this class is an environment suited for reinforcement learning. It has the following methods that are worth noting:\n",
    "\n",
    "`OfficeEnv.step(action)`: Applies an action that transitiones the environment to the next state.\n",
    "\n",
    "`OfficeEnv.reset(seed)`: Resets the environment, returning to the initial state and resetting the random state.\n",
    "\n",
    "`OfficeEnv.render(mode)`: Display the current state of the environment.\n",
    "\n",
    "`OfficeEnv.showmap(mode)`: Display a map of the environment.\n",
    "\n",
    "\n",
    "### The environment has an 'easy' and a 'hard' mode. We will start with the easy one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136da714",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_easy = office_gym.OfficeEnv(mode='easy')\n",
    "env_easy.showmap();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91c8e0f",
   "metadata": {},
   "source": [
    "### A very simple way to solve this problem is to try random actions for a number of episodes and keep the best sequence of actions we encounter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04384fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent():\n",
    "    def __init__(self, actions, seed=None):\n",
    "        self.actions = actions\n",
    "        self.best_action_sequence = []\n",
    "        self.highest_reward_episode = -np.inf\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "    def get_action(self):\n",
    "        return self.rng.choice(self.actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2f8356",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent = RandomAgent(actions=np.arange(4), seed=3)\n",
    "\n",
    "# Number of episodes to search before we time out\n",
    "nepisodes = 100\n",
    "\n",
    "for i in range(nepisodes):\n",
    "    state = env_easy.reset()\n",
    "\n",
    "    reward_episode = 0\n",
    "    done = False\n",
    "    actions = []\n",
    "\n",
    "    while not done:\n",
    "        action = agent.get_action()\n",
    "\n",
    "        state, reward, done, info = env_easy.step(action)\n",
    "\n",
    "        reward_episode += reward\n",
    "        actions += [action]\n",
    "\n",
    "    if reward_episode > agent.highest_reward_episode:\n",
    "        agent.best_action_sequence = actions\n",
    "        agent.highest_reward_episode = reward_episode\n",
    "        print(f'Episode {i}: Found new best path of length {len(actions)} with reward {reward_episode}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63bf640",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reward_episode, steps, animated = office_gym.perform_action_sequence(\n",
    "    agent.best_action_sequence, env_easy, render=True)\n",
    "animated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edab8f2",
   "metadata": {},
   "source": [
    "### Now let us create an agent that estimates the Q-value of each state-action pair it observes\n",
    "\n",
    "For convenience, we repeat the Q-value update function: $Q(s_t, a_t) = Q(s_t, a_t) + \\alpha(R_t + \\gamma \\max_{a}Q(s_{t+1}, a) - Q(s_t, a_t))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b18b797",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class QLearningAgent():\n",
    "    def __init__(self, nstates, actions, learning_rate=0.8, discount_factor=0.99, seed=None):\n",
    "        self.actions = actions\n",
    "        self.qtable = np.zeros((nstates, len(actions)))\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "    def get_action(self, state, epsilon=0):\n",
    "        if self.rng.uniform(0, 1) < epsilon:\n",
    "            action = self.rng.choice(self.actions) # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(self.qtable[state]) # Exploit learned value\n",
    "        return action\n",
    "    \n",
    "    def update_qtable(self, current_state, next_state, action, reward):\n",
    "        old_qvalue = self.qtable[current_state, action]\n",
    "        new_qvalue = (old_qvalue + self.learning_rate*(reward\n",
    "                                                       + self.discount_factor * np.max(self.qtable[next_state])\n",
    "                                                       - old_qvalue))\n",
    "        self.qtable[current_state, action] = new_qvalue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65262762",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent = QLearningAgent(nstates=env_easy.observation_space.n,\n",
    "                       actions=np.arange(env_easy.action_space.n),\n",
    "                       learning_rate=0.5,\n",
    "                       discount_factor=0.99,\n",
    "                       seed=5)\n",
    "\n",
    "epsilon = 0.99\n",
    "\n",
    "for i in range(100):\n",
    "    state = env_easy.reset()\n",
    "\n",
    "    steps, reward = 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.get_action(state, epsilon)\n",
    "        next_state, reward, done, info = env_easy.step(action)\n",
    "        agent.update_qtable(state, next_state, action, reward)\n",
    "        state = next_state\n",
    "    epsilon = max(0.2, epsilon**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07abb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "office_gym.visualize_qvalues(agent, env_easy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7790e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_episode, steps, animated = office_gym.play_episode(agent, env_easy, render=True, seed=None)\n",
    "animated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3d213f",
   "metadata": {},
   "source": [
    "## Spicing up the environment\n",
    "\n",
    "As we all know, printers are not perfect machines, and there is always a risk of a printer not working.\n",
    "\n",
    "We will now study the case where there are two printers in the office. The probability of a printer working varies between printers. In this case the closest printer works 30% of the time, while the printer further away works 90% of the time.\n",
    "\n",
    "If the employee tries to use a printer and it works, they receive a reward of 10. However, if the printer does not work, the employee must return to their desk and make a report in Origo, which yields a reward of -10 (and ends the episode)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419ae955",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_hard = office_gym.OfficeEnv(mode='hard')\n",
    "env_hard.showmap();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba34e320",
   "metadata": {},
   "source": [
    "### Oh no! Someone spilled coffee on the floor!\n",
    "\n",
    "Spaces with coffee spill are slippery, and when moving away from them there is a 1/3 probability of the agent moving in the intended direction, and a 1/3 probability for moving in either of the two perpendicular directions, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c813cc5d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "agent = QLearningAgent(nstates=env_hard.observation_space.n,\n",
    "                       actions=np.arange(env_hard.action_space.n),\n",
    "                       learning_rate=0.01,\n",
    "                       discount_factor=0.9)\n",
    "\n",
    "epsilon = 0.99\n",
    "episodes = 20000\n",
    "ntestepisodes = 100\n",
    "\n",
    "showvalues = [100, 1000, 10000, 20000]\n",
    "\n",
    "for i in range(1, episodes+1):\n",
    "    state = env_hard.reset()\n",
    "\n",
    "    steps, reward = 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.get_action(state, epsilon)\n",
    "        next_state, reward, done, info = env_hard.step(action)\n",
    "        agent.update_qtable(state, next_state, action, reward)\n",
    "        state = next_state\n",
    "    if i in showvalues:\n",
    "        fig = office_gym.visualize_qvalues(agent, env_hard)\n",
    "        fig.suptitle(f'Max Q-values after {i} episodes.')\n",
    "        fig.tight_layout()\n",
    "        \n",
    "    if i % 1000 == 0:\n",
    "        epsilon = max(0.25, epsilon**2)\n",
    "        avg_reward = 0\n",
    "        if ntestepisodes > 0:\n",
    "            for j in range(ntestepisodes):\n",
    "                reward_episode, steps, _ = office_gym.play_episode(agent, env_hard, render=False, seed=None)\n",
    "                avg_reward += reward_episode\n",
    "            avg_reward /= ntestepisodes\n",
    "\n",
    "            print(f'Epoch {i}: Expected episode reward {avg_reward}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7355f44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_episode, steps, animated = office_gym.play_episode(agent, env_hard, render=True, seed=None)\n",
    "animated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4494d722",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
